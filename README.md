# ADS_2021
Portfolio for applied data science minor

Name: Bj√∂rn Appehl <br>
Student ID: 21087024 <br>
Group: Team Dialogue

<h1> Introduction </h1>
This is the reader's guide for my portfolio created for the Applied Data Science minor at The Hague University of Applied Sciences.   <br>
Below are the criteria from the scoring matrix with detailed information about each criteria.   <br>
The reader's guide is numbered for easier reading. <br>
A big thank you to the rest of Team Dialogue, and thanks to to Jeroen, Tony, Ruud and Hani for all your advice in the past six months.

<h2> 1. Reflection and evaluation </h2>

<details>
<summary> 1.1 Reflection on own contribution to the project </summary>

- Situation:  Our project group consisted of 6 members, and we worked with audio data to detect conversation for the Smart Teddy Bear project. We all worked together to ensure everyone would get hands-on experience with every aspect of the project work, although this was hard to realize and in the end some work ended up being unevenly distributed. Since I don't have a great deal of experience writing code, I was a little out of the loop at the end of the project when the code we had for our CNN's became more and more complex. However, at that point I took on other duties which helped the group as a whole but did not give me as much programming experience as some others.

- Task: My tasks in the group varied, early on there was a lot of hands-on with coding simple models. One example is creating a model together with David that ended up being the first real algorithm the group used, since it had the best results at that stage. Later on I started exploring different datasets and drew up some requirements and comparisons for the datasets we ended up using. As the groups priorities shifted, I found myself taking on a lot of presentations and other communication duties along with writing the paper, since we had other people who were simply better at crunching code and it became a matter of time in the final stages. I also helped David & Maria who gave the learning lab feedback and suggestions for topics for them to cover, however I didn't end up taking part in presenting our learning lab. 
  
- Activities: The first model I created in the minor was a Logistic Regression model which was based on transcripts from a TV show. The models purpose was to estimate which line was most likely being said by which speaker. On top of this, I was also splicing audio, normalizing sound levels and transforming our datasets to be more difficult. I helped streamline our data pipeline, unfortunately I finished it right when we shifted to using numpy arrays instead of image data, so it was in the end not necessary. These are only some examples of what I did and you can read more about it below.

- Result: For the presentations I was a part of, I created a lot of the slides along with the overall layout of the powerpoints. I helped other group members in taking care of the Scrum board on Taiga, and during the period in which I was scrummaster I took care of this mostly single-handedly. The code I wrote early on was a simple logistic regression model that was later converted to take audio data as input, however at that point the model also had to change since RFC gave better accuracy. My work on the dataset helped us get good data quite early on the project, which I see as a great benefit for our neural networks.

- Reflect: The contributions I made to the project gave me a much better understanding of data science as a whole. While I am not ready to explore a career in the field, I have a strong feeling that the techniques and methods used in this minor will be of use to me in a professional setting. I regret not being a bigger part of the learning lab our group gave, since it would have been a good chance to expand my own knowledge in the domain. 
 
</details>
  
  

<details>
<summary> 1.2 Reflection on own learning objectives</summary>
  
 - Situation: Since I am studying Business Process Development at my home university, which contains a lot of information about theoretical ICT usage, I wanted to try something more hands on for my exchange. This is part of why I chose the ADS minor, but also since I have always been interested in understanding how algorithms, machine learning and neural networks & such work. In order to put myself in a position where I could learn as much as possible, I did not want to choose a field where I was already well aquainted with the contents. In the group project, we also had to learn about audio data processing a lot. This is something very unexpected, but I'm glad it happened since I now have a much better understanding of audio data processing.
  
 - Task: As a group, we had to figure out a way to use audio data in such a way that we could make predictions on the amount of speakers, and the duration of speech. For the majority of the project, we did not use predetermined roles for our development cycle. Some of my important tasks included: Data cleaning, data transformation, coding the neural networks, giving presentations, and working on our paper. All these tasks helped me understand more about data science as a whole.
  
 - Action: I created machine learning models, such as a CNN and a Linear Regression model, to explore and get a better understanding of the domain which is data science. We all worked with the algorithms, and other than those, I also spent time looking for and creating datasets for our group to use. 
  
 - Result: I ended up getting a very deep understanding of data science during this semester, more so than I thought I would. Our algorithm performed well, and I think this is in part due to all of the group learning from eachother and working in a good pace with little downtime during our productive hours. Working with data science techniques was extremely interesting to me, and I consider my learning goals fulfilled.
  
  
 - Reflection:  All the tasks I completed helped me understand more about data science as a whole. I now consider myself a lot more educated when it comes to data science in general, and my personal goals were achieved. I think the workflow in our group was over expectation, and I am very happy with how the group turned out. One thing I would have improved upon is to stay even more on top of the coding work, since I fell behind a little bit right at the end, due to other group members keeping a very high tempo.
</details>

<details>
<summary> 1.3 Evaluation on the group project as a whole</summary>
  
- Situation:Right from the start, our group contained a lot of different skill sets and this showed during our project. Some were better at writing code, while some had more experience in working with Scrum or other benefitial traits. The cohesion was always quite high in my opinion, and there was never any conflict in the group. Early on, we made it clear what we expect from eachother in terms of workload (i.e not scheduling project work on weekends or after 5pm), punctuality, etc, which helped us work more effectively and better as a group.

- Task: For the duration of the entire project, the workload of all members shifted depending on what stage the project was in. Despite this, some ended up doing a lot more coding than some others, but everyone still partook in presentations and communication along with participating on writing the paper. While everyone did get a little bit of experience in all areas, the workload could have been changed to avoid this. However we wanted to avoid a set schedule with responsibilities in order to not have a member doing something they would rather not do. An unmotivated member working on a task just because it has been assigned to them is not always optimal, in our case we instead focused on everyone doing what they wanted to do based on the current workload at the time.

- Activities: Our application of Scrum consisted of daily online standups, which we had mostly every weekday for the minor unless something else was said. We still had physical standup meetings on days where we were all gathering to work at campus. These 'working days' on campus became quite central in our work, and 2-3 days every week was spent on campus. 

- Results: I, and I belive all other group members, are happy with the results we achieved. Not only are we happy with the algoithm, which gives great results as far as we can see, we also achieved the results working in a sustainable and reasonable pace with little conflict or unnecessary stress. My knowledge about statistics has also increased after taking this minor.

- Reflection: I'm sure none of our group members are finishing this minor without having learned something. The distribution of workload in retrospect was, according to me, a very good way to make sure noone is understimulated or has too much to do. While it took a few weeks to get this running smoothly, mostly due to all members getting to know eachother and their skill sets, it ended up being very benefitial for us. If I were to do this project again, I would happily work with the same group in the same manner as we did. The working days on campus was, according to me, a big factor in our projects success and helped us work better together and make social connections.

  
</details>



<h2> 2. Research Project </h2>

<details>
<summary> 2.1 Task Definition</summary>
  
My contribution: I gave feedback and discussed with the group members (David & Maria, who had created the initial draft) about which research questions we should keep, and which questions we should move forward with. Here is a link to a very early draft of our paper with the questions still in there, the "answers" to each question on page 2 is typed by me and was used for reference later on in the project. 
  https://drive.google.com/file/d/1tm8MRCr17ix6i32tT9nXcVKYS6k9HhKh/view?usp=sharing <br>
  
  I was mostly working on our datasets when our first drafts of the research paper was created, so as soon as I finished work on the dataset I helped out with the questions. Below are some examples of questions that made it, and those that did not (along with our reasoning):
  
 - How can we detect multiple voices from audio data? <br>
This question was central in the project, since the context for our project consists of defining when conversation is happening. Detecting multiple voices makes the difference between a monologue and an actual conversation and is very important for the end result.
  
 - Which characteristics make a conversation?<br>
This question has a lot to do with the one above it. We ultimately decided that a minimum grade of participation from at least two speakers is necessary to constitute a conversation. We had to discuss this with the problem owner several times, as we didn't want to make assumptions ourselves. If we simply regarded any dialogue between people regardless of speech duration per speaker, this could have given different results.
  
 - Can we detect if the dementia patient is speaking on the phone?<br>
This question was considered to be out of scope. There are probably easier ways to determine when an elderly person is using their phone than only listening to them speak, and we wanted to focus on specifically conversation in a physical setting.

- Can we detect if the person speaking is physically present?<br>
This question was ultimately decided to be out of scope, but it did come up for discussion a few times. Essentially, a voice being played from a speaker will most likely not have the same range as a human speaking. This makes it possible - in theory, we never got far enough to actually work on it - to determine when a voice is "fake" or "real". This is a suitable area for further research in my opinion, since we never had time to try it out the results would be very interesting.
</details>


<details>
<summary> 2.2 Evaluation</summary>
My contribution: For the paper, I gave some ideas for future work with our prototype. I put this in the paper so other group members could also put ideas in, and build off mine if they agree. 
  Here is an early draft of our paper where on page 5, my first ideas for future work are listed: https://drive.google.com/file/d/1_IV_NqUBWdRstXnUaXdXFCy66YA4UpWQ/view?usp=sharing
  
  A few of them include:
  
  - Comparing the accuracy of our speaker differentiation model with human results. This could be done by a study where correspondants listen to short clips of speech and asses whether all clips are said by the same speaker or not. It would be very interesting to see if humans or the model perform better if voices are very similar for instance. Since our research has only measured the accuracy of our model,  a "human" accuracy score would be an interesting metric to consider. 

  
  - Since the model for speaker differentation we used came to be quite complex, it would be interesting to see new projects aim to identify the patient's voice as a profile to compare other voices against. Samples might be collected over a period of time and eventually could be used to compare all detected speech to the patient themselves, instead of always comparing every segment with all voices therein. This might result in higher accuracy for determining whether or not it is the patient who is speaking.
  
  - 
</details>
  
  
<details>
<summary> 2.3 Conclusions</summary>
  My conclusions from this project are that it is indeed possible to use data science techniques (in our case, convolutional neural networks) to detect conversation to some degree. By converting audio data to MFCC's, and feeding them through two neural networks, we can with 89% (for detecting speech) and 94% (for detecting changes in speaker) accuracy determine if a conversation is happening. Of course, our algorithm is not perfect, and there will be many situations where it does not work properly. For instance, if the other half of the conversation is taking place over the phone. With the final product, that combines the first and second model, I would say we have results that support our research problem "‚ÄùHow can data science techniques detect if there is a conversation between at least two people by analyzing audio files?‚Äù" and can now state that by using CNNs, MFCC data format and measuring speaker activity & speech duration, data science techniques can detect conversation. The data format of input data can impact the results a lot, which is why we ended up not using images for our final version.
  
</details>
  
  
<details>
<summary> 2.4 Planning</summary>
 My contribution: I, along with Leander Loomans, were in charge of documentation. This included taking notes whenever important information was recieved from teachers or the problem owner (or internal meetings). It also included making documents (internal and external, such as found papers) available for other group members to take part in. On top of this, every group member was equally involved in updating the Scrumboard on Taiga and making sure it was up to date.
  
  
  A screenshot of some of the notes that were taken: https://i.imgur.com/YU2HRXk.png  (Since not every meeting leads to notes having to be taken, there are some gaps.)<br/>
  
  
  
  In our group, efficiency was quite important, and led to us having daily stand up meetings so we could all keep up with the progress happening. This worked very well, and I attended these meetings to increase our group cohesion and keep the others informed about my part in the project. All in all, Scrum as a method worked very well for us in the context that we applied it, and all group members were in charge of project planning to some degree, as the role of Scrummaster rotated in our group. In my eyes, everyone did this job very well, and tasks were evenly spread out among everyone.<br>
  
  
  A screenshot from Taiga with almost everyone's activity: https://i.imgur.com/Q91fnWq.png (It's difficult to get a really descriptive image)
  
  Maria Hoendermis, one of our groupmembers, was very helpful in the planning process and did most of the work in communicating with teachers to set up meetings for the rest of the group.
  
  
</details>


<h2> 3. Predictive Analysis </h2>

<details>
<summary> 3.1 Selecting a Model</summary>
  The decision to use convolutional neural networks was taken early on, and it was very much a group decision. In order to get the best results, we argued that spending a lot of time 'perfecting' one method (we decided on CNN) will lead to better results than spending the same amount of time trying out different models. We looked at literature, such as Ashar, Bhatti and Mushtaq (2020) that use CNNs with MFCCs specifically. This also meant that, since CNN's are able to learn features based on data, we did not have to do much feature extraction/selection. In retrospect, I think the decision to use CNNs was the right thing, but exploring other models would have been interesting, too.
</details>

<details>
<summary> 3.2 Configuring a Model</summary>
  Early on, i configured a neural network (with some help from Jeroen in handling errors). The code can be found here: <br> https://github.com/Digitalswede/ADS_2021/blob/main/codesamples/early_neural_network.py <br/>
My contribution: All of what you see in the notebook, some of the values were changed in accordance with feedback from Jeroen to get things working. As you can see, it is an old version since it uses images for input data. <br/>
  
  I also configured a simple Logistic Regression model early in the course as a first test of machine learning models, using one of the example notebooks provided as the foundation. This file is available here: https://github.com/Digitalswede/ADS_2021/blob/main/codesamples/southpark_test.py
</details>

<details>
<summary> 3.3 Training a Model</summary>
  The models used in our project were trained 
  
</details>

<details>
<summary> 3.4 Evaluating a Model</summary>
  In terms of evaluation, 
  
</details>

<details>
<summary> 3.5 Visualising the outcome of a model</summary>
  
</details>


<h2> 4. Domain Knowledge </h2>  

<details>
<summary> 4.1 Introduction to the subject field </summary>
  As we worked with the Smart Teddy project together with our problem owner Hani, our subject field came to be audio signal processing. This meant we had to use recordings of audio as input to an algorithm in order to make predictions on the audio itself. In order to do this, audio data is transformed into MFCC data, since MFCCs are good at representing a lot of features useful in voice recognition. This process can be seen here, in block [5]: https://github.com/Digitalswede/ADS_2021/blob/main/codesamples/make%20npy%20array%20of%20audio.ipynb <br>
  
  
Sound data can also be represented with spectrograms, and other image representations of sound (such as oscillograms/waveforms). However, we achieved the best results working with MFCCs. The sample rate of recordings is also an important factor to consider, since it is a measure of how many samples are recorded over a period of time. A high sample rate will contain a lot of samples, but might be computationally expensive or contain unnecessarily many samples. While a low sample rate has some information loss, but can be faster to process.
  
All of these techniques mentioned above were relevant in our Dialogue project, which is a part of the bigger Smart Teddy Bear project. This is a very interesting domain, as it contains a lot of different problems. Not only did we need to create a functional algorithm to classify speech, we also needed to work with and get a deep understanding of audio data, and relate all of our work to the healthcare domain for dementia patients. 
  
  
</details>


<details>
<summary> 4.2 Literature Research </summary>
  I found several pieces of relevant literature during this minor. One of the more interesting ones is Udin *et al.* (2018) The topic for their study is Ambient Sensors for Elderly Care, and this study looks at results and data from other works and summarizes their findings. This helped us a lot since in this study, since it gave a good overview of other studies with the same end goal (determine quality of life based on household environment data). From studying this paper, it became apparent that using sound data for the purpose of recognizing daily activity is not as common as some other methods, such as video or infrared sensors. From the study of Udin *et al.* (2018), I found other interesting studies. Such as Vacher *et al.* (2011), a study with some similarities to ours, such as the fact that they are also processing audio data in a household setting for assisted care purposes. Their study mainly relies on audio technologies in smart homes. However, it does not relate to dementia patients, only elderly to some degree.
</details>


<details>
<summary> 4.3 Explaination of Terminology, jargon and definitions </summary>
  
  Below follows an explaination for terms or definitions that are viewed as important:
  - MFC: Mel-Frequency Cepstrum, an aggregation of several MFCC's (coefficients).
  - MFCC : A coefficient to MFC's, meaning one MFC is made up of many MFCCs. MFCCs are a method of displaying features on audio data, and is heavily related to feature extraction.
  - Epoch : An iteration over the entire dataset during the training process for a neural network.
  - Learning Rate : The rate at which a neural network adapts to the data. A learning rate that's too big has a chance to oscillate and "jump over" the optimal solution. This might mean it never reaches a good result. While a learning rate that's too small might take very long to train as the "jumps" it makes are very small.
  - Dataset : A set of data that can be split into train, test and validation parts. Datasets generally consist of negative data (data that is not correct, in our case background noise) and some positive data (in our case speech). Negative and positive data should generally be balanced to avoid algorithms being biased towards one or the other. 
  - Overfitting : Overfitting might occur when a model is trained on a limited data set, and only predicts in accordance with training data instead of adapting to validation or other 'non-training' data.
  - Spectrogram : A visualisation of audio data which highlights changes to sound over time. A spectrogram is generated from a collection of Fourier Transforms, thus creating a more detailed representation of the data.
  - (Machine learning) model: A program that is trained to detect certain patterns in data.
  - Confusion Matrix: A form of evaluation on a model, where the amount of false negatives, false positives and correct estimations are displayed.
  - Sample Rate: An attribute of audio describing the amount of samples over a period of time. A high sample rate is generally good, but might be more computationally expensive. While a low sample rate generally means less samples over time, but can be easier to process.
  - Loss function: A function that is able to determine how the performance of a model relates to the 'true values' of a dataset used. 
  - Neural Network: A type of algorithm that works by using layers containing nodes (also called neurons) that recieve and pass on weighted data in order to make predictions on datasets. Needs to be properly trained in order to work.
  - Outliers: Data points that differ a lot from other data in the set.
  
</details>




<h2> 5. Data Preprocessing </h2>  

<details>
<summary> 5.1 Data Exploration</summary>
  
  In order to familiarize myself with the data we were using, I had to inspect the data to be able to work with it as best as possible.
  One of the instances of data exploration I did is in this notebook: https://github.com/Digitalswede/ADS_2021/blob/main/codesamples/wav%20data%20filter%2Bexploration.ipynb. Here, I started experimenting with using attributes from the data (such as sample rates) while also looking at the labels for our data, and making sure the labels add up with the speech. It was helpful in order to learn about the format of our data, and what our data can be used for. We also based the half-second increments around this information that was retrieved from exploring data.
  
  I also explored the data by looking at it in the software Audacity. Using this software to visualise amplitude of the audio files helped us in selecting data that was well suited to our algorithms. I was primarily looking for data that was not too loud, nor too silent, as not balancing this might mean our algorithm will perform poorly.
  
  
  
</details>

<details>
<summary> 5.2 Data Cleansing</summary>
  
  Some of the data cleansing I did can be found in this notebook, in block [6]: https://github.com/Digitalswede/ADS_2021/blob/main/codesamples/dataset%20incl%20neg%20data.ipynb
 Here, I filter out some specific columns (the ones that will be of use to us) from the 'negativedf' dataframe (this dataframe contains the lables for all negative samples). Afterwards, I concatenate this dataframe with our positive data labels, resulting in a cleaned up version of the negative labels being concatenated to the positive labels.  
  
  I also did some data transformation by overlapping background noises on top of speech. The file I created through this transformation process came to be used a lot, and referred to (internally) as the 'difficult' data set, which we ran through the first model to evaluate its tolerance to speech with overlapping noise. This step was taken again at the end of the project, but then I also amplified the background noises overlaid by 20db, making the dataset even harder for the algorithm. We used this file for evaluation right at the end of the project, and the accuracy from the algorithm (speech detection model) was reduced by around 5%.
  
</details>

<details>
<summary> 5.3 Data Preparation</summary>
While the project was still using images as input data, I created a dataloader to standardize the data preparation process for the group.  Unfortunately this tool never really came to be used since, shortly after I finished it, we switched to not using images anymore as our input.
Some of my work on data prep can be found in this notebook: https://github.com/Digitalswede/ADS_2021/blob/main/codesamples/Standardized%20Image%20Generator.ipynb
  
After the dataloader for images ended up being scrapped due to new requirements, Leander and I created a new version, which was used for the remainder of the project.
It can be found here: https://github.com/Digitalswede/ADS_2021/blob/main/codesamples/make%20npy%20array%20of%20audio.ipynb
For that notebook, I would estimate my contribution is around 30-40%.
  
Luckily we didn't seem to be impacted by outliers or missing values in our data, our results were high enough without accounting for that. As we created the datasets ourselves, we were confident in that data was consistent and uniform. Since we were working with audio data though, this was hard to prove. We also did not find many useful strategies for managing outliers in audio.
  
  
  
</details>

<details>
<summary> 5.4 Data Explaination</summary>
  We used multiple datasets and had to combine them ourselves since our problem owner did not provide data. An important factor for our datasets came to be labeling, which we all spent of time working on. Since the data was not categorical, and we could not label it ourselves in a reliable (or convenient) fashion, all our datasets had to be labeled to describe which parts of the audio contained speech. For the speaker differentation model, this was even more important, as the speakers now had to be labeled too. Luckily we managed to find good, suitable candidates.
  
  
 - AVA-Speech is one of the datasets we used for speech detection. It contains around 45 hours of dialogue from movies, which means it also contains some overlaid background noises. However, the speech is labled, and it is possible by using these labels to only get "CLEAN_SPEECH", which is speech without overlaying noise. We decided to also use the other labels, to train the algorithm and increase its tolerance. We made sure to balance our dataset and have it include a 1:1 amount of true and false data, this was achieved by mixing the data with negative labels. We used 5000 seconds of "SPEECH_WITH_MUSIC", 5000 seconds of "SPEECH_WITH_NOISE", and 5000 seconds of "CLEAN_SPEECH". We combined this with 15000 seconds of "NO_SPEECH", providing us with a total of 30000 seconds of mixed audio data where half is true, and half is false. This dataset is recorded at 44100Hz.
  
  
 - Librispeech, a dataset containing speaker-labled audiobook data, came to be very useful in the project. Since it does not contain (noticeable) noise, this dataset was primarily used for speaker differentiation. This was convineant as all speakers in the dataset are labled. Since this dataset was at 16KHz, we upsampled it to fit the other datasets at 44.1KHz. 
  
  
 - CHIME-Home was used for some negative samples, as it partly contained non-speech audio. This dataset was also at a different sampling rate, and had to be upscaled in order to keep our data integrity as we were not sure how different sampling rates would affect the model.
  
  
  
  
</details>

<details>
<summary> 5.5 Data visualisation (Exploratory) </summary>
I compared visual representations of the data in order to explore the amplitude of certain segments, to decide which segment we should use to train our algorithm. The source data file was too big and would have been very slow to process, so having a visual representation helped us create a smaller but representative version of the dataset. In this instance, the software Audacity was used to visually represent the data while still being able to listen to the audio, for quality reasons (such as spikes in amplitude that may be loud speech, or just a glitch/unintentional sounds from recording). I think there were limited opportunities for us to visualize our data, since we worked specifically to identify speech. 
  
</details>

<h2> 6. Communication </h2>
In this section, my participation in presentations and our paper is highlighted.<br>
This is one area where I feel like I contributed a lot, and I hope the portfolio reflects that. <br>
<br>
<details>
<summary> 6.1 Presentations </summary>
  The presentations where I partook are the following:
  
  External Presentation:
  - #1 (helped create presentation, gave the presentation together with the rest of the group) 
  
  - #2 (created presentation, gave the presentation together with 1 other member)
  
  - #3 (created presentation, gave the presentation together with 1 other member)
  
  Internal Presentation: 
  - #2 (created presentation, gave the presentation together with 1 other member)
  
  - #4 (created presentation, gave the presentation together with 1 other member)
  
  - #5 (created presentation, gave the presentation together with 1 other member)
  
  - #8 (gave the presentation together with 2 others)
  
  - #9 (created the presentation) 

  
</details>


<details>
<summary> 6.2 Writing paper </summary>
  
  I helped write the paper as much as possible. Before the writing started, I gave a detailed overview of our subquestions and answered them, which helped form the base for our paper. Of course the structure changed a lot since then, but it was a start. I worked a lot on the introduction part, including background and research problem of the paper. I also wrote content in other sections, but my primary focus (since we divided it up) was the introduction. The introduction was the first section of the paper I started writing, and I later starter helping out on other sections. But I hope the introduction properly showcases our domain and the purpose of our work.
  
 I also helped other group members writing the paper by giving constructive feedback, always being mindful of other people's work and not criticizing. I ended up making quite a few corrections to the paper in most sections, an effort that I hope changed our paper for the better since I feel it's important to deliver a strong paper. 
  
  It is difficult to give examples here, since writing the paper was a continuous process and quite hard to measure in terms of contribution. 
  
  Here is a link to our finished paper: https://drive.google.com/file/d/1tm8MRCr17ix6i32tT9nXcVKYS6k9HhKh/view?usp=sharing
</details>


<h2> Datacamp </h2>
All datacamp courses except for one were completed. The one that was not completed was **Joining Data with Pandas**, since it seemed like one with least in common with our project work.<br>
Outside of the assigned datacamp courses, roughly 1/4 of another course, **Spoken Language Processing in Python**, was also completed.<br>
Here is a link to a screenshot with the completed courses: https://i.imgur.com/YT92ZKs.png

<h2> References </h2>

A. Ashar, M. S. Bhatti, and U. Mushtaq, (2020), Speaker identification using a hybrid cnn-mfcc approach. in 2020 International Conference on Emerging Trends in Smart Technologies (ICETST), 2020, pp. 1‚Äì4. doi: 10.1109/ICETST49965.2020.9080730. <br/>



M. Vacher, D. Istrate, F. Portet, T. Joubert, and T. Chevalier, (2011), The sweet-home project: audio technology in smart homes to improve well-being and reliance. Annu Int Conf IEEE Eng Med Biol Soc. 2011;2011:5291-4. doi: 10.1109/IEMBS.2011.6091309 PMID: 22255532. <br>


Z. Uddin, W. Khaksar, and J. Torresen, (2018), Ambient Sensors for Elderly Care and Independent Living: A Survey. Sensors 18, no. 7: 2027. https://doi.org/10.3390/s18072027
  

